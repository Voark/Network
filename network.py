"""Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q1vT3He4ZT_n1usObLpQ2t1FVqOLlMpJ
"""

!pip install mlxtend
from mlxtend.data import loadlocal_mnist
#import cupy as cp

x, y = loadlocal_mnist(images_path = r'C:\Users\fredi\PycharmProjects\Network\Data\train-images.idx3-ubyte', labels_path = r'C:\Users\fredi\PycharmProjects\Network\Data\train-labels.idx1-ubyte')

print(type(x))

import numpy as cp
import activation_functions as func

class network():

def __init__(self):
    self.layer_sizes = []
    self.weights = []
    self.biases = []
    self.activation_functions = func.list_functions #need to add functions
    self.activations = []
    self.compiled = False

    def set_input(self, input_size):
        self.input_size = input_size

    def set_output(self, outputs):
        if type(outputs) == int:
            self.outputs = outputs
        else:
            print("Ints only")

    def add_hidden_layers(self, number, size, activation):
        if type(number) == int and type(size) == int and activation_functions.count(activation):
            for i in range(number):
                self.hidden_layer_sizes.append(size)
                self.activations.append(activation)
        else:
            print("Number/size are not ints or activation function doesn't exist")

    def reset_hidden_layers(self):
        self.weights = []
        self.biases = []
        self.compiled = False

    def list_layers(self):
        return self.layer_sizes

    def hidden_layers(self):
        return self.hidden_layers

    def compile_network(self, alpha):
        if not self.compiled:
            try:
                self.weights.append(cp.random.rand(self.layer_sizes[0], self.input_size))
                self.biases.append(cp.random.rand(self.layer_sizes[0]))
            except:
                print("Layer sizes and/or input size not defined")
                self.reset_hidden_layers()
                return
            for i in range(len(self.layer_sizes)):
                self.weights.append(cp.random.rand(self.layer_sizes[i], self.layer_sizes[i-1]))
                self.biases.append(cp.random.rand(self.layer_sizes[i]))
            try:
                self.weights.append(cp.random.rand(self.outputs, self.layer_sizes[-1]))
                self.biases.append(self.outputs)
            except:
                print("Outputs not defined")
                self.reset_hidden_layers()
                return
        self.compiled = True
        self.alpha = alpha
        return
        else:
            print("Already compiled")
            return
#for fun recursion method
#    def __forward_prop(self, x, layer = 0):
#        try:
#            new_x = self.activations[layer](cp.matmul(self.weights[layer], ))
#        except IndexError:
#            return
#            return

    def __forward_prop(self, x):
        inputs = [x]
        for weights, biases, activation_function, input in zip(self.weights, self.biases, self.activations, inputs):
            inputs.append(activation_function(cp.add(cp.matmul(weights, input), biases)))
        return inputs

import sympy
sympy.limit(2/)

a = [1, 2, 3, 4]
b = [3, 4, 5, 6]
c = [1]

for x, y, z in zip(a, b, c):
  print(x, y, z)
  c.append(x)
